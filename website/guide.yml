# Landscape2 guide
#
# This file allows defining the content of the landscape guide.
#
# Reference documentation: https://github.com/cncf/landscape2/blob/main/docs/config/guide.yml

categories:
  - category: "Inference"
    content: |
      Inference refers to the phase where trained machine learning models, particularly LLMs (Large Language Models), are used to generate predictions, outputs, or perform tasks based on new input data.
      It covers all aspects of efficiently serving, routing, executing, and managing models in real-time or batch settings, focusing on scalability, low latency, reliability, and adaptability across different environments and hardware.

    subcategories:
      - subcategory: "Inference Engine"
        content: |
          Inference Engines are optimized frameworks or platforms designed to execute machine learning models, particularly large language models (LLMs), with an emphasis on high performance, low latency, and scalability. These engines allow users to run inferences across various hardware platforms and deploy models efficiently. They handle tasks such as model loading, serving, optimization, and high-throughput execution, making them essential in AI and machine learning applications.

      - subcategory: "Inference Platform"
        content: |
          Inference Platforms are specialized environments or infrastructures designed for deploying and managing large-scale AI inference tasks, particularly for Generative AI (GenAI) models and large language models (LLMs). These platforms focus on providing efficient resource management, scaling capabilities, and ease of use when serving models in production environments, often integrating Kubernetes for orchestration, auto-provisioning, and container-based deployments.

      - subcategory: "AI Gateway"
        content: |
          AI Gateways serve as the central entry point for managing and routing requests to various backend services, including LLMs and AI models. These gateways are designed to handle high traffic volumes, manage APIs, and often provide additional features like rate limiting, security, monitoring, and AI-specific capabilities. They act as intermediaries between clients and services, enabling efficient communication, management, and scalability for AI-driven applications.

      - subcategory: "LLM Router"
        content: |
          LLM routers are specialized systems designed to intelligently route, proxy, or switch between multiple large language model (LLM) providers based on factors like cost, latency, quality, or model capabilities. They help developers and organizations optimize API usage across different backends while maintaining a unified, often OpenAI-compatible, interface. Some also integrate guardrails and evaluation mechanisms to ensure safe and efficient AI interactions.

      - subcategory: "Benchmark"
        content: |
          Benchmark provide standardized methods for evaluating and comparing the performance, quality, and capabilities of LLMs (Large Language Models) and AI systems. They focus on areas such as inference speed, latency, model accuracy, contamination-free testing, and real-world scenario simulation, helping developers and researchers make informed decisions about model and system selection.

      - subcategory: "Output"
        content: |
          Output refers to tools or frameworks that help generate structured or formatted results from large language models (LLMs). These tools are designed to provide well-organized and easily interpretable outputs, especially for complex text generation tasks.


  - category: "Orchestration"
    content: |
      Orchestration in AI involves coordinating and managing the various components, workflows, agents, and services that make up complex AI systems.
      It ensures that models, tools, APIs, agents, and data sources work seamlessly together, supporting scalability, automation, modularity, and efficient deployment from development to production environments.

    subcategories:
      - subcategory: "Workflow"
        content: |
          A workflow provides a structured environment for building, connecting, managing, and deploying AI applications, especially those powered by large language models (LLMs) or other machine learning techniques. These frameworks offer modular components such as model integration, retrieval-augmented generation (RAG) pipelines, agent capabilities, prompt management, observability, and workflow orchestration, enabling rapid prototyping and scalable production deployments.

      - subcategory: "Agent"
        content: |
          An Agent is an autonomous or semi-autonomous system powered by large language models, integrating RAG, Function Calling, and MCP to understand user intent, retrieve external knowledge, invoke tools or APIs, and dynamically execute complex tasks with precise responses.

      - subcategory: "Tool"
        content: |
          A Tool is an external functional module or API invoked by Agents via Function Calling, extending their capabilities to perform specific computations, retrieve real-time data, or interact with third-party services, enabling dynamic task processing and precise outputs.

  - category: "Runtime"
    content: |
      Runtime refers to the infrastructure, environments, and systems that execute AI models, agents, and applications in real-time.
      It provides the foundation for deploying, running, monitoring, and scaling AI workloads, ensuring they perform reliably across cloud, edge, or on-premise setups.
      Runtimes often include components like chat frameworks, databases, code assistants, and observability tools.

    subcategories:
      - subcategory: "Chatbot"
        content: |
          Chatbots are platforms and toolkits designed to build, customize, deploy, and interact with AI chat applications. They often support multiple LLM providers, local deployment, plugin systems, RAG integration, and user-friendly UIs, enabling developers and users to easily create private, extensible, and cross-platform AI assistants.

      - subcategory: "Code Assistant"
        content: |
          Code Assistants are AI-driven tools that support developers throughout the software development lifecycle. They provide capabilities such as intelligent code completion, automated code generation, bug detection, testing assistance, document generation, and integration with development environments and DevOps toolkits. These assistants enhance productivity by understanding project context, codebases, and developer intent.

      - subcategory: "Database"
        content: |
          Databases for AI applications, especially vector databases, are specialized systems designed to store, index, and query high-dimensional embeddings and AI-related data like vectors, text, images, and videos. They enable fast similarity search, retrieval-augmented generation (RAG), and integration with machine learning pipelines.      They often support various data types, including text, images, and videos, and are optimized for high-performance querying and retrieval.

      - subcategory: "Development Environment"
        content: |
          Development Environments for AI focus on providing secure, scalable, and elastic infrastructure to run AI-generated code, develop AI applications, and manage agent workflows. These environments are built to offer seamless cloud runtimes, strong isolation, and flexibility for iterative AI development.

      - subcategory: "Observation"
        content: |
          Observation in AI refers to tools and platforms that help monitor, evaluate, and analyze machine learning models, particularly large language models (LLMs), to ensure optimal performance, identify issues, and manage experiments. These tools often focus on real-time monitoring, logging, and visualization, helping teams improve and maintain their AI systems.


  - category: "Training"
    content: |
      Training is the process of teaching machine learning models to perform specific tasks by exposing them to datasets and optimizing their internal parameters.
      It covers initial model pre-training, fine-tuning on specialized data, alignment with human preferences, and evaluation to ensure high performance and safety.
      Training is critical for creating capable, customized, and robust AI systems across a variety of applications.

    subcategories:
      - subcategory: "Framework"
        content: |
          Framework refers to tools that assist in the training of machine learning models, particularly large AI models. These tools help improve the efficiency, scalability, and accessibility of the training process.

      - subcategory: "FineTune"
        content: |
          Fine-tuning frameworks enable developers and researchers to efficiently adapt pre-trained large language models (LLMs) or vision-language models (VLMs) to specialized tasks or datasets. These tools often provide streamlined pipelines for training, parameter-efficient fine-tuning (PEFT), evaluation, and deployment, with support for both single-modal and multimodal models across a wide range of hardware environments.

      - subcategory: "Alignment"
        content: |
          Alignment in large-scale Agent development adjusts model behavior to align with human values, preferences, and safety constraints using techniques like RLHF and DPO. OpenRLHF leverages PPO, iterative DPO, and LoRA for efficient 70B+ model alignment, ensuring precise and stable outputs. Self-RLHF enhances safety through Constrained Value Alignment and Safe RLHF, minimizing harmful outputs. Integrated with RAG and Function Calling, Alignment ensures Agents remain consistent, reliable, and ethical in complex tasks.

      - subcategory: "Evaluation"
        content: |
          Evaluation frameworks are designed to systematically assess the performance, reliability, and behavior of large language models (LLMs), retrieval-augmented generation (RAG) systems, and AI agents. They provide benchmarking tools, dataset coverage, tracing, and automated evaluation pipelines to support both academic research and production deployment.      They help developers and researchers ensure that AI systems meet quality standards, are free from contamination, and perform reliably in real-world scenarios.

      - subcategory: "Workflow"
        content: |
          Workflow tools orchestrate, manage, and automate the lifecycle of machine learning and AI applications, they are designed to streamline complex pipelines, enabling scalability, reliability, and reproducibility of ML workflows across distributed systems.
